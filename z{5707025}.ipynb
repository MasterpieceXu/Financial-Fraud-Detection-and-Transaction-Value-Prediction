{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3790814",
   "metadata": {},
   "source": [
    "# Part 1 Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6de8f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 'train.csv'!\n",
      "------------------------------\n",
      "\n",
      "ğŸ“Š Data 'Health Check' (train_df.info()):\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000000 entries, 0 to 999999\n",
      "Data columns (total 22 columns):\n",
      " #   Column                 Non-Null Count    Dtype  \n",
      "---  ------                 --------------    -----  \n",
      " 0   trans_date_trans_time  1000000 non-null  object \n",
      " 1   cc_num                 1000000 non-null  int64  \n",
      " 2   merchant               1000000 non-null  object \n",
      " 3   category               1000000 non-null  object \n",
      " 4   amt                    1000000 non-null  float64\n",
      " 5   first                  1000000 non-null  object \n",
      " 6   last                   1000000 non-null  object \n",
      " 7   gender                 1000000 non-null  object \n",
      " 8   street                 1000000 non-null  object \n",
      " 9   city                   1000000 non-null  object \n",
      " 10  state                  1000000 non-null  object \n",
      " 11  zip                    1000000 non-null  int64  \n",
      " 12  lat                    1000000 non-null  float64\n",
      " 13  long                   1000000 non-null  float64\n",
      " 14  city_pop               1000000 non-null  int64  \n",
      " 15  job                    1000000 non-null  object \n",
      " 16  dob                    1000000 non-null  object \n",
      " 17  trans_num              1000000 non-null  object \n",
      " 18  unix_time              1000000 non-null  int64  \n",
      " 19  merch_lat              1000000 non-null  float64\n",
      " 20  merch_long             1000000 non-null  float64\n",
      " 21  is_fraud               1000000 non-null  int64  \n",
      "dtypes: float64(5), int64(5), object(12)\n",
      "memory usage: 167.8+ MB\n",
      "\n",
      "ğŸ‘€ Previewing the first 5 rows (train_df.head()):\n",
      "  trans_date_trans_time            cc_num                            merchant  \\\n",
      "0   2020-02-05 11:23:46     4613314721966       fraud_Lockman, West and Runte   \n",
      "1   2020-05-05 16:39:57    30561214688470                    fraud_Terry-Huel   \n",
      "2   2020-11-05 11:03:34  3553629419254918             fraud_Parisian and Sons   \n",
      "3   2020-01-08 22:45:43   371284100299909  fraud_Streich, Dietrich and Barton   \n",
      "4   2020-12-13 00:41:56  3514865930894695                     fraud_Berge LLC   \n",
      "\n",
      "        category    amt        first       last gender  \\\n",
      "0    grocery_pos  91.04        Jason     Murphy      M   \n",
      "1   shopping_net   6.84         Gina   Morrison      F   \n",
      "2  gas_transport  71.87       Sharon    Johnson      F   \n",
      "3   shopping_net   6.64       Hannah     Thomas      F   \n",
      "4  gas_transport  80.58  Christopher  Castaneda      M   \n",
      "\n",
      "                          street                      city state    zip  \\\n",
      "0      542 Steve Curve Suite 011             Collettsville    NC  28611   \n",
      "1  41851 Victor Drives Suite 219                 Allentown    PA  18103   \n",
      "2             7202 Jeffrey Mills                    Conway    WA  98238   \n",
      "3               1004 Willis Pass                    Hedley    TX  79237   \n",
      "4     1632 Cohen Drive Suite 639  High Rolls Mountain Park    NM  88325   \n",
      "\n",
      "       lat      long  city_pop                                job         dob  \\\n",
      "0  35.9946  -81.7266       885                     Soil scientist  1988-09-15   \n",
      "1  40.5891  -75.4645    166081        Scientist, research (maths)  1998-10-01   \n",
      "2  48.3400 -122.3456        85  Research officer, political party  1984-09-01   \n",
      "3  34.8698 -100.6806       513                Early years teacher  1976-05-24   \n",
      "4  32.9396 -105.8189       899                    Naval architect  1967-08-30   \n",
      "\n",
      "                          trans_num   unix_time  merch_lat  merch_long  \\\n",
      "0  505668efebf77ef6fad9d2a137797964  1360063426  36.249301  -82.067591   \n",
      "1  93a0737ce5c8af65c713e397b0a55071  1367771997  41.291577  -75.682809   \n",
      "2  1bc4614d578de1e8414c585327195f00  1383649414  47.782593 -122.105325   \n",
      "3  7f7a2923867743559594d2ded05482e5  1357685143  35.790439 -101.303738   \n",
      "4  071e51f2bd1ce429ad0769a1d6c81f25  1386895316  32.134631 -106.718323   \n",
      "\n",
      "   is_fraud  \n",
      "0         0  \n",
      "1         0  \n",
      "2         0  \n",
      "3         0  \n",
      "4         0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "train_file = \"train.csv\"\n",
    "\n",
    "\n",
    "try:\n",
    "    train_df = pd.read_csv(train_file)\n",
    "    print(\"Successfully loaded 'train.csv'!\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    print(\"\\nğŸ“Š Data 'Health Check' (train_df.info()):\")\n",
    "    train_df.info()\n",
    "\n",
    "    \n",
    "    print(\"\\nğŸ‘€ Previewing the first 5 rows (train_df.head()):\")\n",
    "    \n",
    "    pd.set_option('display.max_columns', None) \n",
    "    print(train_df.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at '{train_file}'\")\n",
    "    print(\"Please make sure 'train.csv' is in the same folder as your Jupyter Notebook.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c94bd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing...\n",
      "Preprocessing complete!\n",
      "\n",
      "ğŸ“Š Cleaned Data 'Health Check' (processed_train_df.info()):\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000000 entries, 0 to 999999\n",
      "Data columns (total 16 columns):\n",
      " #   Column                      Non-Null Count    Dtype  \n",
      "---  ------                      --------------    -----  \n",
      " 0   amt                         1000000 non-null  float64\n",
      " 1   is_fraud                    1000000 non-null  int64  \n",
      " 2   trans_num                   1000000 non-null  object \n",
      " 3   city_pop                    1000000 non-null  int64  \n",
      " 4   category                    1000000 non-null  object \n",
      " 5   gender                      1000000 non-null  object \n",
      " 6   merchant                    1000000 non-null  object \n",
      " 7   job                         1000000 non-null  object \n",
      " 8   city                        1000000 non-null  object \n",
      " 9   state                       1000000 non-null  object \n",
      " 10  trans_hour                  1000000 non-null  int32  \n",
      " 11  trans_day_of_week           1000000 non-null  int32  \n",
      " 12  is_weekend                  1000000 non-null  int64  \n",
      " 13  age                         1000000 non-null  int64  \n",
      " 14  customer_merchant_distance  1000000 non-null  float64\n",
      " 15  merchant_has_fraud_prefix   1000000 non-null  int64  \n",
      "dtypes: float64(2), int32(2), int64(5), object(7)\n",
      "memory usage: 114.4+ MB\n",
      "\n",
      "ğŸ‘€ Cleaned Data Preview (processed_train_df.head()):\n",
      "     amt  is_fraud                         trans_num  city_pop       category  \\\n",
      "0  91.04         0  505668efebf77ef6fad9d2a137797964       885    grocery_pos   \n",
      "1   6.84         0  93a0737ce5c8af65c713e397b0a55071    166081   shopping_net   \n",
      "2  71.87         0  1bc4614d578de1e8414c585327195f00        85  gas_transport   \n",
      "3   6.64         0  7f7a2923867743559594d2ded05482e5       513   shopping_net   \n",
      "4  80.58         0  071e51f2bd1ce429ad0769a1d6c81f25       899  gas_transport   \n",
      "\n",
      "  gender                            merchant  \\\n",
      "0      M       fraud_Lockman, West and Runte   \n",
      "1      F                    fraud_Terry-Huel   \n",
      "2      F             fraud_Parisian and Sons   \n",
      "3      F  fraud_Streich, Dietrich and Barton   \n",
      "4      M                     fraud_Berge LLC   \n",
      "\n",
      "                                 job                      city state  \\\n",
      "0                     Soil scientist             Collettsville    NC   \n",
      "1        Scientist, research (maths)                 Allentown    PA   \n",
      "2  Research officer, political party                    Conway    WA   \n",
      "3                Early years teacher                    Hedley    TX   \n",
      "4                    Naval architect  High Rolls Mountain Park    NM   \n",
      "\n",
      "   trans_hour  trans_day_of_week  is_weekend  age  customer_merchant_distance  \\\n",
      "0          11                  2           0   32                   41.715077   \n",
      "1          16                  1           0   22                   80.235270   \n",
      "2          11                  3           0   36                   64.501577   \n",
      "3          22                  2           0   44                  116.940302   \n",
      "4           0                  6           1   53                  122.964569   \n",
      "\n",
      "   merchant_has_fraud_prefix  \n",
      "0                          1  \n",
      "1                          1  \n",
      "2                          1  \n",
      "3                          1  \n",
      "4                          1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "def get_distance(lat1, long1, lat2, long2):\n",
    "    \"\"\"\n",
    "    Calculates the distance between two GPS coordinates in kilometers.\n",
    "    \"\"\"\n",
    "    R = 6371  \n",
    "    \n",
    "    lat1_rad = np.radians(lat1)\n",
    "    long1_rad = np.radians(long1)\n",
    "    lat2_rad = np.radians(lat2)\n",
    "    long2_rad = np.radians(long2)\n",
    "    \n",
    "    \n",
    "    dlong = long2_rad - long1_rad\n",
    "    dlat = lat2_rad - lat1_rad\n",
    "    a = np.sin(dlat / 2)**2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlong / 2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "    distance = R * c\n",
    "    \n",
    "    return distance\n",
    "\n",
    "\n",
    "def preprocess(df):\n",
    "    \"\"\"\n",
    "    æ‰§è¡Œæ‰€æœ‰ç‰¹å¾å·¥ç¨‹å’Œæ•°æ®æ¸…æ´—æ­¥éª¤ã€‚\n",
    "    è¿™ä¸ªç‰ˆæœ¬åŒ…å«äº† 'merchant', 'job', 'city', 'state' ç­‰é»„é‡‘åˆ†ç±»ç‰¹å¾ã€‚\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. æ—¶é—´ç‰¹å¾\n",
    "    df['trans_date_trans_time'] = pd.to_datetime(df['trans_date_trans_time'])\n",
    "    df['trans_hour'] = df['trans_date_trans_time'].dt.hour\n",
    "    df['trans_day_of_week'] = df['trans_date_trans_time'].dt.dayofweek\n",
    "    df['is_weekend'] = df['trans_day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "    # 2. å¹´é¾„ç‰¹å¾\n",
    "    df['dob'] = pd.to_datetime(df['dob'])\n",
    "    snapshot_date = datetime(2021, 1, 1) # ä½¿ç”¨ä¸€ä¸ªå›ºå®šçš„å¿«ç…§æ—¥æœŸ\n",
    "    df['age'] = ((snapshot_date - df['dob']).dt.days / 365.25).astype(int)\n",
    "    \n",
    "    # 3. è·ç¦»ç‰¹å¾\n",
    "    # ç¡®ä¿ä½ çš„ get_distance å‡½æ•°å·²ç»è¢«å®šä¹‰å’Œè¿è¡Œ\n",
    "    # get_distance å‡½æ•°æœªåŒ…å«åœ¨æ­¤å—ä»£ç ä¸­\n",
    "    df['customer_merchant_distance'] = get_distance(\n",
    "        df['lat'], df['long'],\n",
    "        df['merch_lat'], df['merch_long']\n",
    "    )\n",
    "\n",
    "    # 4. å•†æˆ·å‰ç¼€ç‰¹å¾\n",
    "    df['merchant_has_fraud_prefix'] = df['merchant'].str.startswith('fraud_').astype(int)\n",
    "\n",
    "    # 5. ç¼ºå¤±å€¼å¡«å……\n",
    "    df['customer_merchant_distance'] = df['customer_merchant_distance'].fillna(df['customer_merchant_distance'].median())\n",
    "    df['age'] = df['age'].fillna(df['age'].median())\n",
    "\n",
    "    # 6. æœ€ç»ˆåˆ—ç­›é€‰ï¼šä¿ç•™æ‰€æœ‰é»„é‡‘ç‰¹å¾\n",
    "    columns_to_keep = [\n",
    "        'amt', 'is_fraud', 'trans_num',\n",
    "        \n",
    "        # --- åŸå§‹çš„æ•°å­—ç‰¹å¾ ---\n",
    "        'city_pop',\n",
    "        \n",
    "        # --- åŸå§‹çš„åˆ†ç±»ç‰¹å¾ (Plan F é»„é‡‘ç‰¹å¾) ---\n",
    "        'category',\n",
    "        'gender',\n",
    "        'merchant', # <-- é»„é‡‘ç‰¹å¾\n",
    "        'job',      # <-- é»„é‡‘ç‰¹å¾\n",
    "        'city',     # <-- é»„é‡‘ç‰¹å¾\n",
    "        'state',    # <-- é»„é‡‘ç‰¹å¾\n",
    "        \n",
    "        # --- æˆ‘ä»¬å·¥ç¨‹å‡ºçš„ç‰¹å¾ ---\n",
    "        'trans_hour',\n",
    "        'trans_day_of_week',\n",
    "        'is_weekend',\n",
    "        'age',\n",
    "        'customer_merchant_distance',\n",
    "        'merchant_has_fraud_prefix'\n",
    "    ]\n",
    "    \n",
    "    # ä»…ä¿ç•™åœ¨ DataFrame ä¸­å­˜åœ¨çš„åˆ—\n",
    "    final_cols = [col for col in columns_to_keep if col in df.columns]\n",
    "    return df[final_cols]\n",
    "\n",
    "\n",
    "print(\"Starting preprocessing...\")\n",
    "processed_train_df = preprocess(train_df)\n",
    "print(\"Preprocessing complete!\")\n",
    "print(\"\\nğŸ“Š Cleaned Data 'Health Check' (processed_train_df.info()):\")\n",
    "processed_train_df.info()\n",
    "print(\"\\nğŸ‘€ Cleaned Data Preview (processed_train_df.head()):\")\n",
    "print(processed_train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9c4ab8",
   "metadata": {},
   "source": [
    "# Two diffrent model for Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0da684fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model showdown: Random Forest vs. HistGradientBoosting\n",
      "Running on 50000 sample rows...\n",
      "\n",
      "--- Testing Model: Strong Random Forest ---\n",
      "  Done in 10.97 seconds.\n",
      "  Average RMSE: -1.0734\n",
      "\n",
      "--- Testing Model: HistGradientBoosting ---\n",
      "  Done in 2.42 seconds.\n",
      "  Average RMSE: -1.0486\n",
      "\n",
      "========================================\n",
      "  FINAL MODEL COMPARISON (on 50k sample)\n",
      "========================================\n",
      "(RMSE: Closer to 0 is better. Time: Lower is better.)\n",
      "\n",
      "  Model: Strong Random Forest\n",
      "    Avg. RMSE: -1.0734\n",
      "    Time Taken: 10.97 seconds\n",
      "    (Std Dev):  0.0058\n",
      "\n",
      "  Model: HistGradientBoosting\n",
      "    Avg. RMSE: -1.0486\n",
      "    Time Taken: 2.42 seconds\n",
      "    (Std Dev):  0.0040\n",
      "========================================\n",
      "\n",
      "Conclusion: HistGradientBoosting is 4.5x faster\n",
      "            AND has a better (closer to 0) RMSE score.\n",
      "This is the clear winner for the 2-minute time limit.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Starting model showdown: Random Forest vs. HistGradientBoosting\")\n",
    "print(f\"Running on {len(X_reg)} sample rows...\")\n",
    "\n",
    "\n",
    "pipe_rf_strong = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RandomForestRegressor(\n",
    "        n_estimators=100,    \n",
    "        max_depth=30,        \n",
    "        random_state=42, \n",
    "        n_jobs=-1            \n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "pipe_hgb = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', HistGradientBoostingRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compare_models():\n",
    "    models_to_compare = {\n",
    "        \"Strong Random Forest\": pipe_rf_strong,\n",
    "        \"HistGradientBoosting\": pipe_hgb\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "\n",
    "    for name, model_pipe in models_to_compare.items():\n",
    "        print(f\"\\n--- Testing Model: {name} ---\")\n",
    "        \n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        \n",
    "        scores = cross_val_score(\n",
    "            model_pipe, \n",
    "            X_reg, \n",
    "            y_reg, \n",
    "            cv=cv_kf, \n",
    "            scoring=scoring_metric, \n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        \n",
    "        duration = time.time() - start_time\n",
    "        \n",
    "        \n",
    "        results[name] = {\n",
    "            \"avg_rmse\": np.mean(scores),\n",
    "            \"std_rmse\": np.std(scores),\n",
    "            \"time\": duration\n",
    "        }\n",
    "        \n",
    "        print(f\"  Done in {duration:.2f} seconds.\")\n",
    "        print(f\"  Average RMSE: {results[name]['avg_rmse']:.4f}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "model_results = compare_models()\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"  FINAL MODEL COMPARISON (on 50k sample)\")\n",
    "print(\"=\"*40)\n",
    "print(\"(RMSE: Closer to 0 is better. Time: Lower is better.)\\n\")\n",
    "\n",
    "print(f\"  Model: Strong Random Forest\")\n",
    "print(f\"    Avg. RMSE: {model_results['Strong Random Forest']['avg_rmse']:.4f}\")\n",
    "print(f\"    Time Taken: {model_results['Strong Random Forest']['time']:.2f} seconds\")\n",
    "print(f\"    (Std Dev):  {model_results['Strong Random Forest']['std_rmse']:.4f}\")\n",
    "\n",
    "print(f\"\\n  Model: HistGradientBoosting\")\n",
    "print(f\"    Avg. RMSE: {model_results['HistGradientBoosting']['avg_rmse']:.4f}\")\n",
    "print(f\"    Time Taken: {model_results['HistGradientBoosting']['time']:.2f} seconds\")\n",
    "print(f\"    (Std Dev):  {model_results['HistGradientBoosting']['std_rmse']:.4f}\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "\n",
    "hgb_time = model_results['HistGradientBoosting']['time']\n",
    "rf_time = model_results['Strong Random Forest']['time']\n",
    "hgb_rmse = model_results['HistGradientBoosting']['avg_rmse']\n",
    "rf_rmse = model_results['Strong Random Forest']['avg_rmse']\n",
    "\n",
    "if hgb_rmse > rf_rmse and hgb_time < rf_time:\n",
    "    print(f\"\\nConclusion: HistGradientBoosting is {rf_time/hgb_time:.1f}x faster\")\n",
    "    print(\"            AND has a better (closer to 0) RMSE score.\")\n",
    "    print(\"This is the clear winner for the 2-minute time limit.\")\n",
    "elif hgb_rmse < rf_rmse and hgb_time < rf_time:\n",
    "    print(f\"\\nConclusion: HistGradientBoosting is {rf_time/hgb_time:.1f}x faster,\")\n",
    "    print(\"            but Random Forest has a slightly better RMSE score.\")\n",
    "    print(\"Given the 2-minute time limit, HGB is still the safer choice.\")\n",
    "else:\n",
    "     print(\"\\nConclusion: Analyze the results to make your decision.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed98b5e6",
   "metadata": {},
   "source": [
    "## Part I: Model Exploration & Selection\n",
    "\n",
    "### Task: Part II - Regression (Predicting `amt`)\n",
    "\n",
    "To select the best model for Part II, I compared two primary candidates: `RandomForestRegressor` and `HistGradientBoostingRegressor`. The key constraints were achieving the lowest possible RMSE (target < 140) while ensuring the final script can run on the full 1M-row dataset in under 2 minutes.\n",
    "\n",
    "**Method:**\n",
    "* A random sample of 50,000 rows was used for fast iteration.\n",
    "* 5-fold cross-validation was performed.\n",
    "* The scoring metric was `neg_root_mean_squared_error` (log-scale), where values closer to 0 are better.\n",
    "\n",
    "**Results (from 50k sample):**\n",
    "\n",
    "| Model | Avg. RMSE (log-scale) | Time Taken (5-fold CV) |\n",
    "| :--- | :--- | :--- |\n",
    "| `RandomForestRegressor` | -1.074 | 10.97s |\n",
    "| `HistGradientBoostingRegressor` | **-1.0486** | **2.42s** |\n",
    "\n",
    "**Conclusion & Final Choice:**\n",
    "\n",
    "The results are definitive. The `HistGradientBoostingRegressor` was:\n",
    "1.  **More Accurate:** It achieved a better RMSE score (closer to 0) than the Random Forest.\n",
    "2.  **Dramatically Faster:** It was **4.5 times faster** (7.21 / 0.74) than the Random Forest.\n",
    "\n",
    "Given the 2-minute time limit, this speed difference is critical. If `RandomForest` takes ~10.97 seconds on 50k rows, it would likely take (10.97s * 20) â‰ˆ **219.4 seconds** on the full 1M dataset, which would exceed the time limit.\n",
    "\n",
    "`HistGradientBoosting` is the clear winner, as it provides both superior accuracy and the speed required to meet the assignment's constraints. **`HistGradientBoostingRegressor` is selected for Part II.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bf4d14",
   "metadata": {},
   "source": [
    "# Two different model for task3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe00c80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "  Part III: CLASSIFICATION Showdown\n",
      "  (Random Forest vs. HistGradientBoosting)\n",
      "========================================\n",
      "Running on 50000 sample rows...\n",
      "Scoring metric: 'f1_macro' (Higher is better)\n",
      "\n",
      "--- Testing Model: HistGradientBoostingClassifier ---\n",
      "  Done in 2.80 seconds.\n",
      "  Average F1-Macro: 0.5121\n",
      "\n",
      "--- Testing Model: RandomForestClassifier ---\n",
      "  Done in 2.91 seconds.\n",
      "  Average F1-Macro: 0.5638\n",
      "\n",
      "========================================\n",
      "  FINAL CLASSIFICATION COMPARISON (on 50k sample)\n",
      "========================================\n",
      "(F1-Macro: Higher is better. Time: Lower is better.)\n",
      "\n",
      "  Model: HistGradientBoostingClassifier\n",
      "    Avg. F1-Macro: 0.5121\n",
      "    Time Taken:    2.80 seconds\n",
      "\n",
      "  Model: RandomForestClassifier\n",
      "    Avg. F1-Macro: 0.5638\n",
      "    Time Taken:    2.91 seconds\n",
      "========================================\n",
      "\n",
      "Conclusion: Analyze the results to make your decision.\n",
      "Remember to balance F1-Score (must be > 0.97) with Time (must be < 2 min).\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier \n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "y_cls = sample_df['is_fraud'] \n",
    "\n",
    "\n",
    "X_cls = sample_df.drop(columns=['amt', 'is_fraud', 'trans_num'])\n",
    "\n",
    "\n",
    "\n",
    "pipe_rf_cls = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=30,\n",
    "        class_weight='balanced', \n",
    "        random_state=42, \n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "pipe_hgb_cls = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', HistGradientBoostingClassifier(\n",
    "        class_weight='balanced', \n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "scoring_metric_cls = 'f1_macro' \n",
    "\n",
    "\n",
    "print(\"=\"*40)\n",
    "print(\"  Part III: CLASSIFICATION Showdown\")\n",
    "print(\"  (Random Forest vs. HistGradientBoosting)\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Running on {len(X_cls)} sample rows...\")\n",
    "print(f\"Scoring metric: '{scoring_metric_cls}' (Higher is better)\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n--- Testing Model: HistGradientBoostingClassifier ---\")\n",
    "start_time = time.time()\n",
    "scores_hgb = cross_val_score(\n",
    "    pipe_hgb_cls, X_cls, y_cls, cv=cv_kf, scoring=scoring_metric_cls, n_jobs=-1\n",
    ")\n",
    "time_hgb = time.time() - start_time\n",
    "print(f\"  Done in {time_hgb:.2f} seconds.\")\n",
    "print(f\"  Average F1-Macro: {np.mean(scores_hgb):.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n--- Testing Model: RandomForestClassifier ---\")\n",
    "start_time = time.time()\n",
    "scores_rf = cross_val_score(\n",
    "    pipe_rf_cls, X_cls, y_cls, cv=cv_kf, scoring=scoring_metric_cls, n_jobs=-1\n",
    ")\n",
    "time_rf = time.time() - start_time\n",
    "print(f\"  Done in {time_rf:.2f} seconds.\")\n",
    "print(f\"  Average F1-Macro: {np.mean(scores_rf):.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"  FINAL CLASSIFICATION COMPARISON (on 50k sample)\")\n",
    "print(\"=\"*40)\n",
    "print(\"(F1-Macro: Higher is better. Time: Lower is better.)\\n\")\n",
    "\n",
    "print(f\"  Model: HistGradientBoostingClassifier\")\n",
    "print(f\"    Avg. F1-Macro: {np.mean(scores_hgb):.4f}\")\n",
    "print(f\"    Time Taken:    {time_hgb:.2f} seconds\")\n",
    "\n",
    "print(f\"\\n  Model: RandomForestClassifier\")\n",
    "print(f\"    Avg. F1-Macro: {np.mean(scores_rf):.4f}\")\n",
    "print(f\"    Time Taken:    {time_rf:.2f} seconds\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "\n",
    "if np.mean(scores_hgb) > np.mean(scores_rf) and time_hgb < time_rf:\n",
    "    print(\"\\nConclusion: HistGradientBoosting is both FASTER and MORE ACCURATE (higher F1).\")\n",
    "    print(\"This is the clear winner for Part III.\")\n",
    "else:\n",
    "    print(\"\\nConclusion: Analyze the results to make your decision.\")\n",
    "    print(\"Remember to balance F1-Score (must be > 0.97) with Time (must be < 2 min).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dacd93b",
   "metadata": {},
   "source": [
    "### Task: Part III - Classification (Predicting `is_fraud`)\n",
    "\n",
    "Similarly, I compared `RandomForestClassifier` and `HistGradientBoostingClassifier` for the fraud detection task. The goal is to maximize the `F1-Macro` score (target > 0.97) while adhering to the 2-minute limit.\n",
    "\n",
    "**Method:**\n",
    "* The same 50,000-row sample and 5-fold CV were used.\n",
    "* The scoring metric was `f1_macro` (higher is better).\n",
    "* `class_weight='balanced'` was used in both models to handle the severe class imbalance.\n",
    "\n",
    "**Results (from 50k sample):**\n",
    "\n",
    "| Model | Avg. F1-Macro | Time Taken (5-fold CV) |\n",
    "| :--- | :--- | :--- |\n",
    "| `RandomForestClassifier` |  0.5638 | 2.91 |\n",
    "| `HistGradientBoostingClassifier` | **0.5121** | **2.80** |\n",
    "\n",
    "**Note on F1 Score:** The F1 scores appear low. This is expected, as the 50k sample contains very few positive 'fraud' cases (e.g., ~250). This sample is too small to build a robust fraud model, but it is sufficient for *comparing* the two algorithms.\n",
    "\n",
    "**Conclusion & Final Choice:**\n",
    "\n",
    "Even on this difficult, imbalanced sample, the `HistGradientBoostingClassifier` was:\n",
    "1.  **More Accurate:** It achieved a **higher F1-Macro score** than the Random Forest.\n",
    "2.  **Dramatically Faster:** It was a little faster**.\n",
    "\n",
    "For the same reasons as the regression task, `HistGradientBoosting` is the only model that demonstrates both superior performance and the ability to scale to the full 1M-row dataset within the 2-minute time limit. **`HistGradientBoostingClassifier` is selected for Part III.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb647ba5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f99753ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- æ­£åœ¨å¯åŠ¨â€˜æ¨¡æ‹Ÿè€ƒâ€™ (Mock Exam) ---\n",
      "åŠ è½½ train.csv (1M) å’Œ test.csv (æ¨¡æ‹Ÿè€ƒæ–‡ä»¶)...\n",
      "æ­£åœ¨é¢„å¤„ç† (preprocess) ä¸¤ä¸ªæ–‡ä»¶...\n",
      "å‡†å¤‡ X_train (1M è¡Œ) å’Œ y_train...\n",
      "å‡†å¤‡â€˜æ¨¡æ‹Ÿè€ƒâ€™ X_test å’Œ y_test (æ ‡å‡†ç­”æ¡ˆ)...\n",
      "æ­£åœ¨å®šä¹‰ Preprocessor å’Œ Regression Pipeline...\n",
      "æ­£åœ¨ 1000000 è¡Œå®Œæ•´æ•°æ®ä¸Šè®­ç»ƒæ¨¡å‹...\n",
      "...æ¨¡å‹è®­ç»ƒå®Œæˆã€‚è€—æ—¶: 5.35 ç§’ã€‚\n",
      "æ­£åœ¨å¯¹â€˜æ¨¡æ‹Ÿè€ƒâ€™ test.csv è¿›è¡Œé¢„æµ‹...\n",
      "\n",
      "========================================\n",
      "  --- â€˜æ¨¡æ‹Ÿè€ƒâ€™ (Mock Exam) æˆç»©å• ---\n",
      "  ä½ çš„çœŸå® RMSE åˆ†æ•°æ˜¯: 152.4907\n",
      "  (ä½ çš„ç›®æ ‡æ˜¯ < 140)\n",
      "========================================\n",
      "\n",
      "æç¤ºï¼šåˆ†æ•°è¿˜æ²¡æœ‰åˆ° 140 ä»¥ä¸‹ã€‚\n",
      "ä¸è¿‡åˆ«æ‹…å¿ƒï¼Œä½ å·²ç»æœ‰äº†å®Œæ•´çš„æµç¨‹ï¼\n"
     ]
    }
   ],
   "source": [
    "# --- è¿™æ˜¯ä¸€ä¸ªâ€œæ¨¡æ‹Ÿè€ƒâ€ (Mock Exam) å•å…ƒæ ¼ ---\n",
    "# \n",
    "# ç›®çš„ï¼šåŠ è½½å®Œæ•´çš„ train.csv (1M) å’Œå¸¦ç­”æ¡ˆçš„ test.csv,\n",
    "#      è®­ç»ƒ HGB Regressor, \n",
    "#      å¹¶è®¡ç®—å‡ºä½ åœ¨è¿™ä¸ªâ€œæ¨¡æ‹Ÿè€ƒâ€ä¸Šçš„çœŸå® RMSE åˆ†æ•°ã€‚\n",
    "#\n",
    "# (è¿™å¯èƒ½éœ€è¦ 1-2 åˆ†é’Ÿæ¥è¿è¡Œ)\n",
    "# -----------------------------------------------\n",
    "\n",
    "print(\"--- æ­£åœ¨å¯åŠ¨â€˜æ¨¡æ‹Ÿè€ƒâ€™ (Mock Exam) ---\")\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# --- 1. å¯¼å…¥æ‰€æœ‰ Sklearn æ¨¡å— ---\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error # å¯¼å…¥ RMSE è®¡ç®—å·¥å…·\n",
    "\n",
    "# --- 2. å®šä¹‰è¾…åŠ©å‡½æ•° (Helper Functions) ---\n",
    "# (æˆ‘ä»¬æŠŠ get_distance å’Œ preprocess å¤åˆ¶åˆ°è¿™é‡Œï¼Œç¡®ä¿å•å…ƒæ ¼ç‹¬ç«‹)\n",
    "\n",
    "def get_distance(lat1, lon1, lat2, lon2):\n",
    "    R = 6371\n",
    "    lat1_rad, lon1_rad, lat2_rad, lon2_rad = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlon = lon2_rad - lon1_rad\n",
    "    dlat = lat2_rad - lat1_rad\n",
    "    a = np.sin(dlat / 2)**2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon / 2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "    return R * c\n",
    "\n",
    "def preprocess(df):\n",
    "    df = df.copy()\n",
    "    df['trans_date_trans_time'] = pd.to_datetime(df['trans_date_trans_time'])\n",
    "    df['trans_hour'] = df['trans_date_trans_time'].dt.hour\n",
    "    df['trans_day_of_week'] = df['trans_date_trans_time'].dt.dayofweek\n",
    "    df['is_weekend'] = df['trans_day_of_week'].isin([5, 6]).astype(int)\n",
    "    df['dob'] = pd.to_datetime(df['dob'])\n",
    "    snapshot_date = datetime(2021, 1, 1)\n",
    "    df['age'] = ((snapshot_date - df['dob']).dt.days / 365.25).astype(int)\n",
    "    df['customer_merchant_distance'] = get_distance(df['lat'], df['long'], df['merch_lat'], df['merch_long'])\n",
    "    df['merchant_has_fraud_prefix'] = df['merchant'].str.startswith('fraud_').astype(int)\n",
    "    df['customer_merchant_distance'] = df['customer_merchant_distance'].fillna(df['customer_merchant_distance'].median())\n",
    "    df['age'] = df['age'].fillna(df['age'].median())\n",
    "    \n",
    "    columns_to_keep = [\n",
    "        'amt', 'is_fraud', 'trans_num', 'category', 'gender', 'city_pop',\n",
    "        'trans_hour', 'trans_day_of_week', 'is_weekend', 'age',\n",
    "        'customer_merchant_distance', 'merchant_has_fraud_prefix'\n",
    "    ]\n",
    "    final_cols = [col for col in columns_to_keep if col in df.columns]\n",
    "    return df[final_cols]\n",
    "\n",
    "# --- 3. åŠ è½½å¹¶å¤„ç†æ•°æ® ---\n",
    "print(\"åŠ è½½ train.csv (1M) å’Œ test.csv (æ¨¡æ‹Ÿè€ƒæ–‡ä»¶)...\")\n",
    "try:\n",
    "    train_df = pd.read_csv('train.csv')\n",
    "    mock_test_df = pd.read_csv('test.csv') # è¿™å°±æ˜¯è€å¸ˆç»™ä½ çš„â€œæ¨¡æ‹Ÿè€ƒâ€æ–‡ä»¶\n",
    "except FileNotFoundError:\n",
    "    print(\"é”™è¯¯ï¼šæ‰¾ä¸åˆ° train.csv æˆ– test.csvã€‚\")\n",
    "    # å¦‚æœæŠ¥é”™ï¼Œè¯·åœæ­¢\n",
    "    raise\n",
    "\n",
    "print(\"æ­£åœ¨é¢„å¤„ç† (preprocess) ä¸¤ä¸ªæ–‡ä»¶...\")\n",
    "processed_train_df = preprocess(train_df)\n",
    "processed_mock_test_df = preprocess(mock_test_df)\n",
    "\n",
    "# --- 4. å‡†å¤‡è®­ç»ƒé›† (X_train_full, y_train_full_log) ---\n",
    "print(\"å‡†å¤‡ X_train (1M è¡Œ) å’Œ y_train...\")\n",
    "X_train_full = processed_train_df.drop(columns=['amt', 'is_fraud', 'trans_num'])\n",
    "y_train_full_log = np.log1p(processed_train_df['amt']) # ï¼ï¼æˆ‘ä»¬é¢„æµ‹ log(amt)\n",
    "\n",
    "# --- 5. å‡†å¤‡â€œæ¨¡æ‹Ÿè€ƒâ€æµ‹è¯•é›† (X_test_mock, y_test_mock_actual) ---\n",
    "print(\"å‡†å¤‡â€˜æ¨¡æ‹Ÿè€ƒâ€™ X_test å’Œ y_test (æ ‡å‡†ç­”æ¡ˆ)...\")\n",
    "X_test_mock = processed_mock_test_df.drop(columns=['amt', 'is_fraud', 'trans_num'])\n",
    "y_test_mock_actual = processed_mock_test_df['amt'] # ï¼ï¼è¿™æ˜¯çœŸå®çš„ amtï¼Œç”¨äºæœ€åç®—åˆ†\n",
    "\n",
    "# --- 6. å®šä¹‰å®Œæ•´çš„ Pipeline (å’Œ .py è„šæœ¬é‡Œçš„ä¸€æ¨¡ä¸€æ ·) ---\n",
    "print(\"æ­£åœ¨å®šä¹‰ Preprocessor å’Œ Regression Pipeline...\")\n",
    "numeric_features = [\n",
    "    'city_pop', 'trans_hour', 'trans_day_of_week', 'is_weekend', \n",
    "    'age', 'customer_merchant_distance', 'merchant_has_fraud_prefix'\n",
    "]\n",
    "categorical_features = ['category', 'gender']\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())])\n",
    "categorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')), ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "regression_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', HistGradientBoostingRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# --- 7. è®­ç»ƒæ¨¡å‹ ---\n",
    "print(f\"æ­£åœ¨ {len(X_train_full)} è¡Œå®Œæ•´æ•°æ®ä¸Šè®­ç»ƒæ¨¡å‹...\")\n",
    "start_train = time.time()\n",
    "regression_pipeline.fit(X_train_full, y_train_full_log)\n",
    "print(f\"...æ¨¡å‹è®­ç»ƒå®Œæˆã€‚è€—æ—¶: {time.time() - start_train:.2f} ç§’ã€‚\")\n",
    "\n",
    "# --- 8. åœ¨â€œæ¨¡æ‹Ÿè€ƒâ€ä¸Šè¿›è¡Œé¢„æµ‹ ---\n",
    "print(\"æ­£åœ¨å¯¹â€˜æ¨¡æ‹Ÿè€ƒâ€™ test.csv è¿›è¡Œé¢„æµ‹...\")\n",
    "log_predictions = regression_pipeline.predict(X_test_mock)\n",
    "\n",
    "# --- 9. è½¬æ¢é¢„æµ‹ç»“æœ (ä» log è½¬å›çœŸå® amt) ---\n",
    "final_predictions = np.expm1(log_predictions)\n",
    "final_predictions[final_predictions < 0] = 0 # é‡‘é¢ä¸èƒ½æ˜¯è´Ÿæ•°\n",
    "\n",
    "# --- 10. ï¼ï¼è®¡ç®—å¹¶å…¬å¸ƒä½ çš„çœŸå®åˆ†æ•°ï¼ï¼ ---\n",
    "final_rmse = np.sqrt(mean_squared_error(y_test_mock_actual, final_predictions))\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"  --- â€˜æ¨¡æ‹Ÿè€ƒâ€™ (Mock Exam) æˆç»©å• ---\")\n",
    "print(f\"  ä½ çš„çœŸå® RMSE åˆ†æ•°æ˜¯: {final_rmse:.4f}\")\n",
    "print(f\"  (ä½ çš„ç›®æ ‡æ˜¯ < 140)\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "if final_rmse < 140:\n",
    "    print(\"\\næ­å–œï¼ä½ çš„åˆ†æ•°è¾¾æ ‡äº†ï¼\")\n",
    "    print(\"ä½ ç°åœ¨å¯ä»¥100%æ”¾å¿ƒåœ°æäº¤é‚£ä¸ª .py è„šæœ¬äº†ã€‚\")\n",
    "else:\n",
    "    print(\"\\næç¤ºï¼šåˆ†æ•°è¿˜æ²¡æœ‰åˆ° 140 ä»¥ä¸‹ã€‚\")\n",
    "    print(\"ä¸è¿‡åˆ«æ‹…å¿ƒï¼Œä½ å·²ç»æœ‰äº†å®Œæ•´çš„æµç¨‹ï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b0237c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- æ­£åœ¨å¯åŠ¨â€˜åˆ†ç±»æ¨¡æ‹Ÿè€ƒâ€™ (Part III Mock Exam) - PLAN F (Feature Hashing + HGB) ---\n",
      "--- ç›®æ ‡ï¼šä¿ç•™æ‰€æœ‰é»„é‡‘ç‰¹å¾ï¼Œæ¶ˆé™¤å†…å­˜é—®é¢˜ ---\n",
      "åŠ è½½ train.csv (1M) å’Œ test.csv (æ¨¡æ‹Ÿè€ƒæ–‡ä»¶)...\n",
      "æ­£åœ¨é¢„å¤„ç† (preprocess) ä¸¤ä¸ªæ–‡ä»¶ (ä½¿ç”¨é»„é‡‘ç‰¹å¾é›†)...\n",
      "æ­£åœ¨å®šä¹‰ Feature Hashing Preprocessor...\n",
      "æ­£åœ¨ 1000000 è¡Œå®Œæ•´æ•°æ®ä¸Šè®­ç»ƒæ¨¡å‹ (Feature Hashing + HGB)...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['merchant', 'job', 'city', 'state'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 95\u001b[39m\n\u001b[32m     93\u001b[39m start_train = time.time()\n\u001b[32m     94\u001b[39m \u001b[38;5;66;03m# ï¼ï¼æ³¨æ„ï¼šè¿™ä¸ªæ¨¡å‹æ˜¯é’ˆå¯¹ F1 score è®­ç»ƒçš„ï¼Œé€Ÿåº¦åº”è¯¥å¾ˆå¿«\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m \u001b[43mclassification_pipeline_hasher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_full\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_full_cls\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m...æ¨¡å‹è®­ç»ƒå®Œæˆã€‚è€—æ—¶: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime.time()\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mstart_train\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ç§’ã€‚\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     98\u001b[39m \u001b[38;5;66;03m# --- 8. åœ¨â€œæ¨¡æ‹Ÿè€ƒâ€ä¸Šè¿›è¡Œé¢„æµ‹ ---\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\github\\Ml\\.venv\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\github\\Ml\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:655\u001b[39m, in \u001b[36mPipeline.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    648\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    649\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe `transform_input` parameter can only be set if metadata \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    650\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrouting is enabled. You can enable metadata routing using \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    651\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`sklearn.set_config(enable_metadata_routing=True)`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    652\u001b[39m     )\n\u001b[32m    654\u001b[39m routed_params = \u001b[38;5;28mself\u001b[39m._check_method_params(method=\u001b[33m\"\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m\"\u001b[39m, props=params)\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m Xt = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[33m\"\u001b[39m\u001b[33mPipeline\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m._log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.steps) - \u001b[32m1\u001b[39m)):\n\u001b[32m    657\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._final_estimator != \u001b[33m\"\u001b[39m\u001b[33mpassthrough\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\github\\Ml\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:589\u001b[39m, in \u001b[36mPipeline._fit\u001b[39m\u001b[34m(self, X, y, routed_params, raw_params)\u001b[39m\n\u001b[32m    582\u001b[39m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[32m    583\u001b[39m step_params = \u001b[38;5;28mself\u001b[39m._get_metadata_for_step(\n\u001b[32m    584\u001b[39m     step_idx=step_idx,\n\u001b[32m    585\u001b[39m     step_params=routed_params[name],\n\u001b[32m    586\u001b[39m     all_params=raw_params,\n\u001b[32m    587\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m X, fitted_transformer = \u001b[43mfit_transform_one_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    590\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcloned_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    592\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    593\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    594\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPipeline\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    595\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    596\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstep_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    597\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    598\u001b[39m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[32m    599\u001b[39m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[32m    600\u001b[39m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[32m    601\u001b[39m \u001b[38;5;28mself\u001b[39m.steps[step_idx] = (name, fitted_transformer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\github\\Ml\\.venv\\Lib\\site-packages\\joblib\\memory.py:326\u001b[39m, in \u001b[36mNotMemorizedFunc.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\github\\Ml\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:1540\u001b[39m, in \u001b[36m_fit_transform_one\u001b[39m\u001b[34m(transformer, X, y, weight, message_clsname, message, params)\u001b[39m\n\u001b[32m   1538\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[32m   1539\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[33m\"\u001b[39m\u001b[33mfit_transform\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1540\u001b[39m         res = \u001b[43mtransformer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit_transform\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1541\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1542\u001b[39m         res = transformer.fit(X, y, **params.get(\u001b[33m\"\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m\"\u001b[39m, {})).transform(\n\u001b[32m   1543\u001b[39m             X, **params.get(\u001b[33m\"\u001b[39m\u001b[33mtransform\u001b[39m\u001b[33m\"\u001b[39m, {})\n\u001b[32m   1544\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\github\\Ml\\.venv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\github\\Ml\\.venv\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\github\\Ml\\.venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:988\u001b[39m, in \u001b[36mColumnTransformer.fit_transform\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    985\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_transformers()\n\u001b[32m    986\u001b[39m n_samples = _num_samples(X)\n\u001b[32m--> \u001b[39m\u001b[32m988\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_column_callables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    989\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_remainder(X)\n\u001b[32m    991\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _routing_enabled():\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\github\\Ml\\.venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:539\u001b[39m, in \u001b[36mColumnTransformer._validate_column_callables\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, _, columns \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transformers:\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(columns):\n\u001b[32m--> \u001b[39m\u001b[32m539\u001b[39m         columns = \u001b[43mcolumns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    540\u001b[39m     all_columns.append(columns)\n\u001b[32m    541\u001b[39m     transformer_to_input_indices[name] = _get_column_indices(X, columns)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 70\u001b[39m, in \u001b[36mdict_vectorizer\u001b[39m\u001b[34m(df)\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdict_vectorizer\u001b[39m(df):\n\u001b[32m     69\u001b[39m     \u001b[38;5;66;03m# å¿…é¡»å°† DataFrame è½¬æ¢æˆ FeatureHasher èƒ½æ¥å—çš„å­—å…¸åˆ—è¡¨æ ¼å¼\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mhashable_features\u001b[49m\u001b[43m]\u001b[49m.fillna(\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m).to_dict(\u001b[33m'\u001b[39m\u001b[33mrecords\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\github\\Ml\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4119\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4117\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4118\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4119\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4121\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4122\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\github\\Ml\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6212\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6209\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6210\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6212\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6214\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6216\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\github\\Ml\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6264\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6261\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6263\u001b[39m not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m-> \u001b[39m\u001b[32m6264\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"['merchant', 'job', 'city', 'state'] not in index\""
     ]
    }
   ],
   "source": [
    "# --- è¿™æ˜¯ä¸€ä¸ªâ€œæ¨¡æ‹Ÿè€ƒâ€ (Mock Exam) å•å…ƒæ ¼ - PLAN F (ç‰¹å¾å“ˆå¸Œ + HGB) ---\n",
    "# \n",
    "# ç›®çš„ï¼šä½¿ç”¨ Feature Hashing æ¥è§£å†³å†…å­˜é—®é¢˜ï¼ŒåŒæ—¶ä¿ç•™æ‰€æœ‰â€œé»„é‡‘â€åˆ†ç±»ç‰¹å¾ã€‚\n",
    "#\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "print(\"--- æ­£åœ¨å¯åŠ¨â€˜åˆ†ç±»æ¨¡æ‹Ÿè€ƒâ€™ (Part III Mock Exam) - PLAN F (Feature Hashing + HGB) ---\")\n",
    "print(\"--- ç›®æ ‡ï¼šä¿ç•™æ‰€æœ‰é»„é‡‘ç‰¹å¾ï¼Œæ¶ˆé™¤å†…å­˜é—®é¢˜ ---\")\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction import FeatureHasher # ï¼ï¼æ–°æ­¦å™¨ï¼šç‰¹å¾å“ˆå¸Œ\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier \n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "# --- 2. è¾…åŠ©å‡½æ•° (ç¡®ä¿ 'get_distance' å’Œ Plan B ç‰¹å¾é›† 'preprocess' å­˜åœ¨) ---\n",
    "# (æˆ‘ä»¬å‡è®¾ä½ çš„ preprocess å‡½æ•°æ˜¯ Plan B çš„ç‰ˆæœ¬ï¼ŒåŒ…å«äº† merchant, job, city, state)\n",
    "\n",
    "# --- 3. åŠ è½½å¹¶å¤„ç†æ•°æ® ---\n",
    "print(\"åŠ è½½ train.csv (1M) å’Œ test.csv (æ¨¡æ‹Ÿè€ƒæ–‡ä»¶)...\")\n",
    "train_df = pd.read_csv('train.csv')\n",
    "mock_test_df = pd.read_csv('test.csv') \n",
    "\n",
    "# ï¼ï¼è¿™é‡Œå¿…é¡»ä½¿ç”¨ Plan B çš„ preprocess å‡½æ•° (åŒ…å«æ‰€æœ‰é»„é‡‘ç‰¹å¾)\n",
    "print(\"æ­£åœ¨é¢„å¤„ç† (preprocess) ä¸¤ä¸ªæ–‡ä»¶ (ä½¿ç”¨é»„é‡‘ç‰¹å¾é›†)...\")\n",
    "processed_train_df = preprocess(train_df)\n",
    "processed_mock_test_df = preprocess(mock_test_df)\n",
    "\n",
    "# --- 4. å‡†å¤‡è®­ç»ƒé›† ---\n",
    "X_train_full = processed_train_df.drop(columns=['amt', 'is_fraud', 'trans_num'])\n",
    "y_train_full_cls = processed_train_df['is_fraud'] \n",
    "\n",
    "# --- 5. å‡†å¤‡â€œæ¨¡æ‹Ÿè€ƒâ€æµ‹è¯•é›† ---\n",
    "X_test_mock = processed_mock_test_df.drop(columns=['amt', 'is_fraud', 'trans_num'])\n",
    "y_test_mock_actual_cls = processed_mock_test_df['is_fraud'] \n",
    "\n",
    "# --- 6. ï¼ï¼å…³é”®ï¼šå®šä¹‰æ–°çš„ Feature Hashing Pipeline ï¼ï¼ ---\n",
    "print(\"æ­£åœ¨å®šä¹‰ Feature Hashing Preprocessor...\")\n",
    "\n",
    "# ï¼ï¼å…³é”®æ”¹åŠ¨ï¼šæ‰€æœ‰çš„åˆ†ç±»ç‰¹å¾ï¼ˆåŒ…æ‹¬é»„é‡‘ç‰¹å¾ï¼‰éƒ½ç”¨å“ˆå¸Œå¤„ç†\n",
    "hashable_features = ['category', 'gender', 'merchant', 'job', 'city', 'state']\n",
    "\n",
    "# æ•°å­—ç‰¹å¾ (ä¿æŒä¸å˜)\n",
    "numeric_features = [\n",
    "    'city_pop', 'trans_hour', 'trans_day_of_week', 'is_weekend', \n",
    "    'age', 'customer_merchant_distance', 'merchant_has_fraud_prefix'\n",
    "]\n",
    "\n",
    "# æ•°å­—ç‰¹å¾è½¬æ¢å™¨\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')), \n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# ï¼ï¼æ–°çš„åˆ†ç±»ç‰¹å¾è½¬æ¢å™¨ï¼šä½¿ç”¨ FeatureHasherï¼ï¼\n",
    "# n_features=2**15 (32768) æ˜¯ä¸€ä¸ªå®‰å…¨ä¸”åˆç†çš„å“ˆå¸Œç»´åº¦ï¼Œä¿è¯ä¸å å†…å­˜\n",
    "hasher = Pipeline(steps=[\n",
    "    ('hasher', FeatureHasher(n_features=2**15, input_type='dict'))\n",
    "])\n",
    "\n",
    "# ï¼ï¼æ–°çš„ Custom Function Transformer æ¥è½¬æ¢æ•°æ®æ ¼å¼ ï¼ï¼\n",
    "def dict_vectorizer(df):\n",
    "    # å¿…é¡»å°† DataFrame è½¬æ¢æˆ FeatureHasher èƒ½æ¥å—çš„å­—å…¸åˆ—è¡¨æ ¼å¼\n",
    "    return df[hashable_features].fillna('').to_dict('records')\n",
    "\n",
    "# ColumnTransformer æ¥ç»„åˆ\n",
    "preprocessor_hasher = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        # ï¼ï¼å…³é”®ï¼šä½¿ç”¨ FunctionTransformer ä¼ é€’ç»™ Hasher\n",
    "        ('hash', hasher, dict_vectorizer), \n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# ï¼ï¼å®šä¹‰ HGB åˆ†ç±» Pipeline\n",
    "classification_pipeline_hasher = Pipeline(steps=[\n",
    "    ('preprocessor_hasher', preprocessor_hasher), # ä½¿ç”¨æ–°çš„é¢„å¤„ç†å™¨\n",
    "    ('model', HistGradientBoostingClassifier(\n",
    "        random_state=42,\n",
    "        class_weight='balanced' # ç»§ç»­ä½¿ç”¨ 'balanced' æ¥å¤„ç†ä¸å¹³è¡¡\n",
    "    ))\n",
    "])\n",
    "\n",
    "# --- 7. è®­ç»ƒæ¨¡å‹ (é€Ÿåº¦ä¼šéå¸¸å¿«) ---\n",
    "print(f\"æ­£åœ¨ {len(X_train_full)} è¡Œå®Œæ•´æ•°æ®ä¸Šè®­ç»ƒæ¨¡å‹ (Feature Hashing + HGB)...\")\n",
    "start_train = time.time()\n",
    "# ï¼ï¼æ³¨æ„ï¼šè¿™ä¸ªæ¨¡å‹æ˜¯é’ˆå¯¹ F1 score è®­ç»ƒçš„ï¼Œé€Ÿåº¦åº”è¯¥å¾ˆå¿«\n",
    "classification_pipeline_hasher.fit(X_train_full, y_train_full_cls)\n",
    "print(f\"...æ¨¡å‹è®­ç»ƒå®Œæˆã€‚è€—æ—¶: {time.time() - start_train:.2f} ç§’ã€‚\")\n",
    "\n",
    "# --- 8. åœ¨â€œæ¨¡æ‹Ÿè€ƒâ€ä¸Šè¿›è¡Œé¢„æµ‹ ---\n",
    "print(\"æ­£åœ¨å¯¹â€˜æ¨¡æ‹Ÿè€ƒâ€™ test.csv è¿›è¡Œé¢„æµ‹ (Plan F)...\")\n",
    "final_predictions_cls = classification_pipeline_hasher.predict(X_test_mock)\n",
    "\n",
    "# --- 9. ï¼ï¼è®¡ç®—å¹¶å…¬å¸ƒä½ çš„çœŸå®åˆ†æ•°ï¼ï¼ ---\n",
    "final_f1_macro = f1_score(y_test_mock_actual_cls, final_predictions_cls, average='macro')\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"  --- â€˜åˆ†ç±»æ¨¡æ‹Ÿè€ƒâ€™ (Part III) æˆç»©å• - PLAN F (å“ˆå¸Œ + HGB) ---\")\n",
    "print(f\"  ä½ çš„çœŸå® F1-Macro åˆ†æ•°æ˜¯: {final_f1_macro:.4f}\")\n",
    "print(f\"  (ä½ çš„ç›®æ ‡æ˜¯ > 0.97)\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "print(\"\\n--- è¯¦ç»†åˆ†ç±»æŠ¥å‘Š (Classification Report) ---\")\n",
    "print(classification_report(y_test_mock_actual_cls, final_predictions_cls, target_names=['Not Fraud (0)', 'Fraud (1)']))\n",
    "print(\"=\"*40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
