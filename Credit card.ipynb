{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2db4d6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "REFERENCE_DATE = pd.to_datetime('2021-01-01') \n",
    "\n",
    "def preprocess_basic(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Dataclean„ÄÅtransfer data type and create time and age Feature.\n",
    "    Which is the basic of age Feature.\n",
    "    \n",
    "    Args:\n",
    "        df: \"train.csv\" DataFrame.\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: train.csv after preprocessing.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    \n",
    "    df['trans_num'] = df['trans_num'].astype(str)\n",
    "    df['trans_date_trans_time'] = pd.to_datetime(df['trans_date_trans_time'])\n",
    "    df['dob'] = pd.to_datetime(df['dob'])\n",
    "    \n",
    "    \n",
    "    df = df.sort_values('trans_date_trans_time').reset_index(drop=True)\n",
    "\n",
    "    \n",
    "    df['trans_hour'] = df['trans_date_trans_time'].dt.hour\n",
    "    df['trans_day'] = df['trans_date_trans_time'].dt.dayofweek # 0 is monday, and 6 is sunday.\n",
    "    df['trans_month'] = df['trans_date_trans_time'].dt.month\n",
    "    df['trans_year'] = df['trans_date_trans_time'].dt.year\n",
    "    \n",
    "    # Count user age\n",
    "    # Refernce date is 2021-01-01\n",
    "    df['age'] = (REFERENCE_DATE - df['dob']).dt.days // 365\n",
    "    \n",
    "    \n",
    "    cols_to_drop = [          \n",
    "        'first', 'last', 'street', 'dob', \n",
    "        'unix_time'        \n",
    "    ]\n",
    "    \n",
    "    df = df.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df687eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    # Distance \n",
    "    R=6317\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    # Haversine \n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    \n",
    "    # distance\n",
    "    distance = R * c\n",
    "    return distance\n",
    "\n",
    "def feature_engineering(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Distance Metrics„ÄÅCard-level Rolling Statistics / Temporal Features, and Category/Merchant Amount Aggregations.\n",
    "    \n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    df['merch_haversine_dist'] = haversine_distance( \n",
    "        df['lat'], df['long'], df['merch_lat'], df['merch_long']\n",
    "    )\n",
    "    df = df.drop(columns=['lat', 'long', 'merch_lat', 'merch_long'], errors='ignore')\n",
    "    \n",
    "    \n",
    "    # Card-level Rolling Statistics / Temporal Features\n",
    "    \n",
    "    # Transfer  time to Unix (seconds)\n",
    "    df['unix_time_sec'] = df['trans_date_trans_time'].astype(np.int64) // 10**9\n",
    "    \n",
    "    # Temporal Feature\n",
    "    df['cc_time_since_last'] = df.groupby('cc_num')['unix_time_sec'].diff().fillna(999999) \n",
    "    \n",
    "    \n",
    "    # Cumulative Statistics\n",
    "    df['cc_count_cum'] = df.groupby('cc_num').cumcount()\n",
    "    \n",
    "    df['cc_mean_amt_cum'] = df.groupby('cc_num')['amt'].transform(\n",
    "        lambda x: x.shift(1).expanding().mean().fillna(0)\n",
    "    )\n",
    "    \n",
    "    first_time = df.groupby('cc_num')['unix_time_sec'].transform('min')\n",
    "    df['cc_time_diff_total'] = df['unix_time_sec'] - first_time + 1 \n",
    "    df['cc_freq'] = df['cc_count_cum'] / df['cc_time_diff_total']\n",
    "    \n",
    "    \n",
    "    # Category/Merchant Amount Aggregations\n",
    "    \n",
    "    \n",
    "    \n",
    "    df['category_mean_amt'] = df.groupby('category')['amt'].transform('mean')\n",
    "    df['amt_vs_cat_mean'] = df['amt'] / df['category_mean_amt']\n",
    "    \n",
    "\n",
    "    df['amt_per_pop'] = df['amt'] / (df['city_pop'] + 1)\n",
    "    \n",
    "\n",
    "    df = df.drop(columns=['city', 'state', 'zip', 'unix_time_sec', 'cc_time_diff_total', 'cc_num', 'merchant'], errors='ignore')\n",
    "    \n",
    "    \n",
    "    return df\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "825efaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "test_df =pd.read_csv('test.csv')\n",
    "df_full = feature_engineering(preprocess_basic(train_df))\n",
    "df_test_proc = feature_engineering(preprocess_basic(test_df))\n",
    "\n",
    "TARGET_REG = 'amt'\n",
    "# Exclude target, classification target, IDs, and original timestamp\n",
    "FEATURES_REG = [col for col in df_full.columns if col not in [TARGET_REG, 'is_fraud', 'trans_num', 'trans_date_trans_time']]\n",
    "\n",
    "X_reg = df_full[FEATURES_REG]\n",
    "y_reg = df_full[TARGET_REG]\n",
    "\n",
    "# Define Feature Types and Preprocessor ---\n",
    "# These feature lists must strictly correspond to the output of your feature_engineering function\n",
    "numeric_features = [\n",
    "    'city_pop', 'age', 'merch_haversine_dist', 'cc_time_since_last', \n",
    "    'cc_mean_amt_cum', 'cc_count_cum', 'cc_freq', \n",
    "    'category_mean_amt', 'amt_vs_cat_mean', 'amt_per_pop'\n",
    "] \n",
    "categorical_features = [\n",
    "    'category', 'gender', 'job', 'trans_hour', 'trans_day', \n",
    "    'trans_month', 'trans_year' \n",
    "] \n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "preprocessor_reg = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough' \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add3723b",
   "metadata": {},
   "source": [
    "# Task 2 Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35d47a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Comparing Ridge Regression (Baseline) ---\n",
      "Ridge CV RMSE: 39.10\n",
      "\n",
      "--- Comparing LightGBM Regressor (High Performance/Efficiency) ---\n",
      "LGBMR CV RMSE: 77.46\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## ### 1. Model Comparison: Ridge vs. LGBMR (Part I Requirement)\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Cross-validation setup\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "SAFE_N_JOBS = 2\n",
    "\n",
    "# Define RMSE evaluation function (for cross_val_score)\n",
    "def evaluate_rmse_cv(model, X, y, cv):\n",
    "    \"\"\"\n",
    "    Evaluates the model's RMSE performance using K-Fold cross-validation.\n",
    "    \"\"\"\n",
    "    # cross_val_score ‰ΩøÁî® 'neg_mean_squared_error' \n",
    "    scores = cross_val_score(model, X, y, cv=cv, \n",
    "                             scoring='neg_mean_squared_error', n_jobs=SAFE_N_JOBS)\n",
    "    rmse_scores = np.sqrt(-scores)\n",
    "    return rmse_scores.mean(), rmse_scores.std()\n",
    "\n",
    "# Model A: Ridge Regression (Baseline) ---\n",
    "ridge_model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_reg),\n",
    "    ('regressor', Ridge(alpha=10.0, random_state=42)) \n",
    "])\n",
    "\n",
    "# Model B: LightGBM Regressor (High Performance/Efficiency) ---\n",
    "lgbm_reg_model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_reg),\n",
    "    # Reduced n_estimators to 50 to ensure speed\n",
    "    ('regressor', LGBMRegressor(n_estimators=50, max_depth=5, learning_rate=0.1, random_state=42, n_jobs=1))\n",
    "    \n",
    "])\n",
    "\n",
    "print(\"--- Comparing Ridge Regression (Baseline) ---\")\n",
    "ridge_rmse_mean, _ = evaluate_rmse_cv(ridge_model, X_reg, y_reg, cv)\n",
    "print(f\"Ridge CV RMSE: {ridge_rmse_mean:.2f}\")\n",
    "\n",
    "print(\"\\n--- Comparing LightGBM Regressor (High Performance/Efficiency) ---\")\n",
    "lgbm_reg_rmse_mean, _ = evaluate_rmse_cv(lgbm_reg_model, X_reg, y_reg, cv)\n",
    "print(f\"LGBMR CV RMSE: {lgbm_reg_rmse_mean:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644ba6b4",
   "metadata": {},
   "source": [
    "### 2. Part II Regression Task: Model Selection and Comparison\n",
    "\n",
    "**Objective:** Predict the transaction amount (`amt`), with the performance metric being $RMSE \\le 140$.\n",
    "\n",
    "To perform efficient and robust model selection, we compared a linear model (Ridge Regression) against a high-performance ensemble tree model (LightGBM Regressor), utilizing **3-Fold Cross-Validation (CV)** as the primary evaluation method.\n",
    "\n",
    "| Model | Role | CV RMSE Mean | Conclusion |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Ridge Regression** | Baseline Model | $39.10$ | **Exceptional performance, far exceeding the $\\le 140$ target.** |\n",
    "| **LightGBM Regressor** | High-Performance Model | $77.46$ | Excellent performance, but inferior to Ridge. |\n",
    "\n",
    "#### Rationale and Final Choice: Ridge Regression\n",
    "\n",
    "Based on the cross-validation results, **Ridge Regression** achieved the lowest RMSE of $39.10$, demonstrating performance significantly superior to LightGBM.\n",
    "\n",
    "* **Technical Insight:** This suggests that the aggregate features constructed during our feature engineering process (such as `category_mean_amt` and `amt_vs_cat_mean`) have an **extremely strong, nearly linear correlation** with the target variable, `amt`.\n",
    "* **Model Selection:** As a regularized linear model, Ridge is capable of capturing this linear relationship with minimal complexity, maximal speed, and high stability.\n",
    "* **Conclusion:** We select **Ridge Regression** as the final model for Part II."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "764b5817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Ridge Regression Hyperparameter Tuning\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "\n",
      "Best Ridge Model CV RMSE: 39.19\n",
      "Best parameter alpha: 500.0\n"
     ]
    }
   ],
   "source": [
    "## ### 3. Hyperparameter Tuning: Optimizing Ridge Regression's alpha\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "print(\"\\n## Ridge Regression Hyperparameter Tuning\")\n",
    "\n",
    "# Tuning the alpha parameter (L2 regularization strength)\n",
    "param_grid_ridge = {\n",
    "    'regressor__alpha': [1.0, 10.0, 100.0, 500.0]\n",
    "}\n",
    "\n",
    "ridge_grid_search = GridSearchCV(\n",
    "    ridge_model, \n",
    "    param_grid_ridge, \n",
    "    cv=cv, # Using 3-fold Cross-Validation\n",
    "    scoring='neg_mean_squared_error', \n",
    "    n_jobs=SAFE_N_JOBS, \n",
    "    verbose=1 \n",
    ")\n",
    "\n",
    "ridge_grid_search.fit(X_reg, y_reg)\n",
    "\n",
    "best_rmse_ridge = np.sqrt(-ridge_grid_search.best_score_)\n",
    "best_alpha = ridge_grid_search.best_params_['regressor__alpha']\n",
    "\n",
    "print(f\"\\nBest Ridge Model CV RMSE: {best_rmse_ridge:.2f}\")\n",
    "print(f\"Best parameter alpha: {best_alpha}\")\n",
    "\n",
    "# Recording the final parameter\n",
    "FINAL_REG_ALPHA = best_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2ee4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Performance Report ---\n",
      "Final Ridge Model RMSE on test.csv: 29.93\n",
      "‚úÖ Part II Goal Achieved: Final model performance (29.93) meets the RMSE <= 140 requirement.\n"
     ]
    }
   ],
   "source": [
    "## ### 4. Final Performance Confirmation (Using Public Validation Set test.csv)\n",
    "\n",
    "# 1. Create the final model (using the optimal alpha\n",
    "final_ridge_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_reg),\n",
    "    ('regressor', Ridge(alpha=FINAL_REG_ALPHA, random_state=42))\n",
    "])\n",
    "\n",
    "# 2. Train on the entire training set (train_csv)\n",
    "final_ridge_pipeline.fit(X_reg, y_reg)\n",
    "\n",
    "# 3. Predict on test.csv\n",
    "X_test_reg = df_test_proc.drop(columns=[TARGET_REG, 'is_fraud', 'trans_num', 'trans_date_trans_time'], errors='ignore')\n",
    "y_test_reg_true = df_test_proc[TARGET_REG] \n",
    "test_predictions = final_ridge_pipeline.predict(X_test_reg)\n",
    "test_predictions[test_predictions < 0] = 0 \n",
    "\n",
    "# 4. Calculate RMSE on test.csv\n",
    "final_rmse_on_test = np.sqrt(mean_squared_error(y_test_reg_true, test_predictions))\n",
    "\n",
    "print(f\"\\n--- Final Performance Report ---\")\n",
    "print(f\"Final Ridge Model RMSE on test.csv: {final_rmse_on_test:.2f}\")\n",
    "\n",
    "# Final conclusion\n",
    "if final_rmse_on_test <= 140:\n",
    "    print(f\"‚úÖ Part II Goal Achieved: Final model performance ({final_rmse_on_test:.2f}) meets the RMSE <= 140 requirement.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Warning: Final performance did not meet the target.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774a61df",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e4bb4136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 3 Target Variable y_train_cls prepared. Fraud ratio: 0.5220%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "TARGET_CLS = 'is_fraud'\n",
    "y_train_cls = df_full[TARGET_CLS] \n",
    "\n",
    "\n",
    "X_train = X_reg \n",
    "\n",
    "\n",
    "SAFE_N_JOBS = 2\n",
    "\n",
    "print(f\"Task 3 Target Variable y_train_cls prepared. Fraud ratio: {y_train_cls.mean():.4%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38415f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Comparing Logistic Regression (Baseline) ---\n",
      "LR CV F1-Macro: 0.5087 (+/- 0.0014)\n",
      "\n",
      "--- Comparing LightGBM Classifier + SMOTE (Selected) ---\n",
      "LGBMC + SMOTE CV F1-Macro: 0.8814 (+/- 0.0048)\n"
     ]
    }
   ],
   "source": [
    "## ### 2. Part III Classification Task: Model Comparison\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier \n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "f1_macro_scorer = make_scorer(f1_score, average='macro')\n",
    "# Using Stratified 3-fold for speed and class balance handling\n",
    "cv_cls = StratifiedKFold(n_splits=3, shuffle=True, random_state=42) \n",
    "\n",
    "\n",
    "def evaluate_f1_macro_cv(model, X, y, cv):\n",
    "    \"\"\"\n",
    "    Evaluates the model's F1-Macro performance using Stratified K-Fold CV.\n",
    "    \"\"\"\n",
    "    scores = cross_val_score(model, X, y, cv=cv, \n",
    "                             scoring=f1_macro_scorer, n_jobs=SAFE_N_JOBS)\n",
    "    return scores.mean(), scores.std()\n",
    "\n",
    "# --- Model C: Logistic Regression (Baseline) ---\n",
    "lr_model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_reg), \n",
    "    ('classifier', LogisticRegression(solver='liblinear', class_weight='balanced', random_state=42, max_iter=1000))\n",
    "])\n",
    "\n",
    "# --- Model D: LightGBM Classifier + SMOTE (High Performance/Efficiency) ---\n",
    "lgbm_model_smote = ImbPipeline(steps=[\n",
    "    ('preprocessor', preprocessor_reg), \n",
    "    ('smote', SMOTE(sampling_strategy='minority', random_state=42)), \n",
    "    ('classifier', LGBMClassifier(n_estimators=200, random_state=42, n_jobs=1, verbose=-1)) \n",
    "])\n",
    "\n",
    "print(\"--- Comparing Logistic Regression (Baseline) ---\")\n",
    "lr_f1_mean, lr_f1_std = evaluate_f1_macro_cv(lr_model, X_train, y_train_cls, cv_cls)\n",
    "print(f\"LR CV F1-Macro: {lr_f1_mean:.4f} (+/- {lr_f1_std:.4f})\")\n",
    "\n",
    "print(\"\\n--- Comparing LightGBM Classifier + SMOTE (Selected) ---\")\n",
    "lgbm_f1_mean, lgbm_f1_std = evaluate_f1_macro_cv(lgbm_model_smote, X_train, y_train_cls, cv_cls)\n",
    "print(f\"LGBMC + SMOTE CV F1-Macro: {lgbm_f1_mean:.4f} (+/- {lgbm_f1_std:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1af55497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Logistic Regression (Baseline) ---\n",
      "‚úÖ LR CV F1-Macro: 0.5087 (+/- 0.0014) | Time: 30.60 s\n",
      "\n",
      "--- 2. LightGBM (N_EST=200) - Final Performance Target ---\n",
      "‚úÖ LGBM CV F1-Macro: 0.9217 (+/- 0.0021)\n",
      " ¬† Time: 133.30 s (Single Est: 44.43 s)\n",
      "\n",
      "--- 3. XGBoost (N_EST=50) - Speed Risk Assessment ---\n",
      "‚úÖ XGBoost CV F1-Macro: 0.9022 (+/- 0.0049)\n",
      " ¬† Time: 375.02 s (Single Est: 125.01 s)\n",
      "\n",
      "--- 4. LightGBM (N_EST=50) - Speed Risk Baseline ---\n",
      "‚úÖ LGBM CV F1-Macro: 0.8165 (+/- 0.0038)\n",
      " ¬† Time: 76.54 s (Single Est: 25.51 s)\n",
      "\n",
      "--- Final Decision Basis ---\n",
      "XGBoost single training time is 125.01 seconds, confirming it's too slow for the target N_EST.\n",
      "The final script must use LightGBM, as its strongest configuration has a safe single training time of 44.43 seconds.\n"
     ]
    }
   ],
   "source": [
    "## ### 2. Part III Final Model Comparison (LR vs LGBM vs XGBoost)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier \n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.pipeline import Pipeline # Standard Pipeline for LR\n",
    "\n",
    "# --- Configuration ---\n",
    "f1_macro_scorer = make_scorer(f1_score, average='macro')\n",
    "cv_cls = StratifiedKFold(n_splits=3, shuffle=True, random_state=42) \n",
    "\n",
    "# --- Shared Parameters ---\n",
    "N_ESTIMATORS_HIGH = 200 # N_ESTIMATORS for high performance evaluation\n",
    "N_ESTIMATORS_RISK = 50  # N_ESTIMATORS for speed risk assessment\n",
    "SMOTE_SAMPLING_RISK = 0.5 # SMOTE sampling ratio for risk assessment\n",
    "\n",
    "\n",
    "def evaluate_f1_macro_cv_timed(model, X, y, cv):\n",
    "    \"\"\"\n",
    "    Evaluates the model's F1-Macro performance, standard deviation, and 3-Fold CV total time.\n",
    "    \"\"\"\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    # Perform 3-Fold CV using cross_val_score\n",
    "    scores = cross_val_score(model, X, y, cv=cv, \n",
    "                             scoring=f1_macro_scorer, n_jobs=SAFE_N_JOBS)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    mean_f1 = scores.mean()\n",
    "    std_f1 = scores.std()\n",
    "    total_time = end_time - start_time\n",
    "    single_time = total_time / 3\n",
    "    \n",
    "    return mean_f1, std_f1, total_time, single_time\n",
    "\n",
    "\n",
    "# 1. Model Definition\n",
    "\n",
    "\n",
    "# 1.1 Model LR: Logistic Regression (Baseline)\n",
    "lr_model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_reg), \n",
    "    ('classifier', LogisticRegression(solver='liblinear', class_weight='balanced', random_state=42, max_iter=1000))\n",
    "])\n",
    "\n",
    "# 1.2 Model LGBM-High: LightGBM (n_estimators=200) \n",
    "lgbm_high_model = ImbPipeline(steps=[\n",
    "    ('preprocessor', preprocessor_reg), \n",
    "    ('smote', SMOTE(sampling_strategy='minority', random_state=42)), \n",
    "    ('classifier', LGBMClassifier(n_estimators=N_ESTIMATORS_HIGH, random_state=42, n_jobs=1, verbose=-1)) \n",
    "])\n",
    "\n",
    "# 1.3 Model XGB-Risk: XGBoost (n_estimators=50) \n",
    "xgb_risk_model = ImbPipeline(steps=[\n",
    "    ('preprocessor', preprocessor_reg), \n",
    "    ('smote', SMOTE(sampling_strategy=SMOTE_SAMPLING_RISK, random_state=42)), \n",
    "    ('classifier', XGBClassifier(n_estimators=N_ESTIMATORS_RISK, use_label_encoder=False, \n",
    "                                 eval_metric='logloss', random_state=42, n_jobs=1)) \n",
    "])\n",
    "\n",
    "# 1.4 Model LGBM-Risk: LightGBM (n_estimators=50) \n",
    "lgbm_risk_model = ImbPipeline(steps=[\n",
    "    ('preprocessor', preprocessor_reg), \n",
    "    ('smote', SMOTE(sampling_strategy=SMOTE_SAMPLING_RISK, random_state=42)), \n",
    "    ('classifier', LGBMClassifier(n_estimators=N_ESTIMATORS_RISK, random_state=42, n_jobs=1, verbose=-1)) \n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# 2. Model Execution and Output\n",
    "\n",
    "\n",
    "print(\"--- 1. Logistic Regression (Baseline) ---\")\n",
    "lr_f1, lr_std, lr_time, _ = evaluate_f1_macro_cv_timed(lr_model, X_train, y_train_cls, cv_cls)\n",
    "print(f\"‚úÖ LR CV F1-Macro: {lr_f1:.4f} (+/- {lr_std:.4f}) | Time: {lr_time:.2f} s\")\n",
    "\n",
    "print(\"\\n--- 2. LightGBM (N_EST=200) - Final Performance Target ---\")\n",
    "lgbm_high_f1, lgbm_high_std, lgbm_high_time, lgbm_high_single = evaluate_f1_macro_cv_timed(lgbm_high_model, X_train, y_train_cls, cv_cls)\n",
    "print(f\"‚úÖ LGBM CV F1-Macro: {lgbm_high_f1:.4f} (+/- {lgbm_high_std:.4f})\")\n",
    "print(f\" ¬† Time: {lgbm_high_time:.2f} s (Single Est: {lgbm_high_single:.2f} s)\")\n",
    "\n",
    "print(\"\\n--- 3. XGBoost (N_EST=50) - Speed Risk Assessment ---\")\n",
    "xgb_risk_f1, xgb_risk_std, xgb_risk_time, xgb_risk_single = evaluate_f1_macro_cv_timed(xgb_risk_model, X_train, y_train_cls, cv_cls)\n",
    "print(f\"‚úÖ XGBoost CV F1-Macro: {xgb_risk_f1:.4f} (+/- {xgb_risk_std:.4f})\")\n",
    "print(f\" ¬† Time: {xgb_risk_time:.2f} s (Single Est: {xgb_risk_single:.2f} s)\")\n",
    "\n",
    "print(\"\\n--- 4. LightGBM (N_EST=50) - Speed Risk Baseline ---\")\n",
    "lgbm_risk_f1, lgbm_risk_std, lgbm_risk_time, lgbm_risk_single = evaluate_f1_macro_cv_timed(lgbm_risk_model, X_train, y_train_cls, cv_cls)\n",
    "print(f\"‚úÖ LGBM CV F1-Macro: {lgbm_risk_f1:.4f} (+/- {lgbm_risk_std:.4f})\")\n",
    "print(f\" ¬† Time: {lgbm_risk_time:.2f} s (Single Est: {lgbm_risk_single:.2f} s)\")\n",
    "\n",
    "print(\"\\n--- Final Decision Basis ---\")\n",
    "print(f\"XGBoost single training time is {xgb_risk_single:.2f} seconds, confirming it's too slow for the target N_EST.\")\n",
    "print(f\"The final script must use LightGBM, as its strongest configuration has a safe single training time of {lgbm_high_single:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f6ad6502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Final OOF Threshold Optimization (N_est=500, 5-Fold)...\n",
      "‚úÖ OOF Probability Generation Time: 166.80 seconds\n",
      "\n",
      "--- Best OOF Threshold Results ---\n",
      "New Model's Best OOF F1-Macro: 0.9528\n",
      "New Best Threshold: 0.3310\n"
     ]
    }
   ],
   "source": [
    "## ### Final Tuning: Step 1: OOF Threshold Optimization (N_est=500)\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# --- Lock High Performance Parameters ---\n",
    "FINAL_CLS_ESTIMATORS = 500 \n",
    "MAX_DEPTH_FINAL = 15\n",
    "NUM_LEAVES_FINAL = 70\n",
    "FINAL_SMOTE_SAMPLING = 0.05\n",
    "LEARNING_RATE_FINAL = 0.05\n",
    "\n",
    "cv_threshold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# 1. Create OOF Model Pipeline \n",
    "# Note: cross_val_predict runs 5 separate fits, which takes a long time\n",
    "oof_lgbm_pipeline = ImbPipeline(steps=[\n",
    "    ('preprocessor', preprocessor_reg),\n",
    "    ('smote', SMOTE(sampling_strategy=FINAL_SMOTE_SAMPLING, random_state=42)),\n",
    "    ('classifier', LGBMClassifier(\n",
    "        n_estimators=FINAL_CLS_ESTIMATORS,\n",
    "        max_depth=MAX_DEPTH_FINAL,\n",
    "        num_leaves=NUM_LEAVES_FINAL,\n",
    "        learning_rate=LEARNING_RATE_FINAL,\n",
    "        random_state=42, n_jobs=1, verbose=-1))\n",
    "])\n",
    "\n",
    "print(f\"Running Final OOF Threshold Optimization (N_est={FINAL_CLS_ESTIMATORS}, 5-Fold)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# 2. Generate OOF Prediction Probabilities\n",
    "oof_proba = cross_val_predict(\n",
    "    oof_lgbm_pipeline, X_train, y_train_cls,\n",
    "    cv=cv_threshold, method=\"predict_proba\", n_jobs=SAFE_N_JOBS\n",
    ")[:, 1]\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"‚úÖ OOF Probability Generation Time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "# 3. Find Best Threshold\n",
    "thresholds = np.linspace(0.001, 0.999, 500)\n",
    "f1_scores = [f1_score(y_train_cls, (oof_proba >= t).astype(int), average='macro') for t in thresholds]\n",
    "\n",
    "best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "best_f1_oof = np.max(f1_scores)\n",
    "\n",
    "print(f\"\\n--- Best OOF Threshold Results ---\")\n",
    "print(f\"New Model's Best OOF F1-Macro: {best_f1_oof:.4f}\")\n",
    "print(f\"New Best Threshold: {best_threshold:.4f}\")\n",
    "\n",
    "# Record the new threshold\n",
    "NEW_FINAL_CLS_THRESHOLD = best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9d181684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Final LightGBM Model (N_est=500, SMOTE=0.05)...\n",
      "‚úÖ Final Model Training Time: 54.06 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\github\\Ml\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Performance Report (Test Set) ---\n",
      "Final LGBM Model F1-Macro on test.csv: 0.9107\n",
      "‚ö†Ô∏è **Final Gap:** Remaining difference to target F1=0.9700: **0.0593**.\n",
      "‚úÖ Final Conclusion: This is the best generalizing F1 Score under the strict 2-minute time limit.\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier \n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# --- ÊúÄÁªàÈîÅÂÆöÁöÑÊúÄ‰Ω≥ÂèÇÊï∞ ---\n",
    "FINAL_CLS_THRESHOLD = 0.3310      # Optimal Threshold\n",
    "FINAL_CLS_ESTIMATORS = 500      # Safe performance limit for speed\n",
    "MAX_DEPTH_FINAL = 15             # Anti-overfitting depth\n",
    "NUM_LEAVES_FINAL = 70           # Anti-overfitting number of leaves\n",
    "FINAL_SMOTE_SAMPLING = 0.05     # Latest best SMOTE ratio\n",
    "LEARNING_RATE_FINAL = 0.05\n",
    "# 1. Create Final Model Pipeline\n",
    "final_lgbm_pipeline = ImbPipeline(steps=[\n",
    "    ('preprocessor', preprocessor_reg),\n",
    "    ('smote', SMOTE(sampling_strategy=FINAL_SMOTE_SAMPLING, random_state=42)), \n",
    "    ('classifier', LGBMClassifier(\n",
    "        n_estimators=FINAL_CLS_ESTIMATORS, \n",
    "        max_depth=MAX_DEPTH_FINAL,        \n",
    "        num_leaves=NUM_LEAVES_FINAL,      \n",
    "        random_state=42, n_jobs=1, verbose=-1))\n",
    "])\n",
    "\n",
    "# 2. Train on the entire training set (Core time-consuming part of the final script)\n",
    "print(f\"Training Final LightGBM Model (N_est={FINAL_CLS_ESTIMATORS}, SMOTE={FINAL_SMOTE_SAMPLING})...\")\n",
    "start_time = time.time()\n",
    "# Ensure X_train, y_train_cls are defined\n",
    "final_lgbm_pipeline.fit(X_train, y_train_cls)\n",
    "end_time = time.time()\n",
    "print(f\"‚úÖ Final Model Training Time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# 3. Predict on test.csv\n",
    "# Assuming X_test_cls variable is correctly defined (i.e., contains features of test.csv)\n",
    "test_proba = final_lgbm_pipeline.predict_proba(X_test_cls)[:, 1]\n",
    "\n",
    "# 4. Apply the optimized threshold for classification\n",
    "test_predictions_cls = (test_proba >= FINAL_CLS_THRESHOLD).astype(int)\n",
    "\n",
    "# 5. Calculate F1 Score on test.csv (Assuming test.csv contains the true 'is_fraud' labels)\n",
    "if 'is_fraud' in df_test_proc.columns:\n",
    "    y_test_cls_true = df_test_proc['is_fraud']\n",
    "    final_f1_on_test = f1_score(y_test_cls_true, test_predictions_cls, average='macro')\n",
    "    \n",
    "    print(f\"\\n--- Final Performance Report (Test Set) ---\")\n",
    "    print(f\"Final LGBM Model F1-Macro on test.csv: {final_f1_on_test:.4f}\")\n",
    "    \n",
    "    # Report gap to target\n",
    "    target_f1 = 0.97\n",
    "    difference = target_f1 - final_f1_on_test\n",
    "    \n",
    "    if final_f1_on_test >= target_f1:\n",
    "        print(f\"üéâüéâ Part III Goal Achieved! Final F1 Score: {final_f1_on_test:.4f}.\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è **Final Gap:** Remaining difference to target F1={target_f1:.4f}: **{difference:.4f}**.\")\n",
    "        print(f\"‚úÖ Final Conclusion: This is the best generalizing F1 Score under the strict 2-minute time limit.\")\n",
    "FINAL_CLS_PREDICTIONS = test_predictions_cls\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
